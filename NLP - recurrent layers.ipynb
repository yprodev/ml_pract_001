{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights_0_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86b40dad6d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_0_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights_0_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormed_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_0_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights_0_1' is not defined"
     ]
    }
   ],
   "source": [
    "# continuation of the previous NLP part\n",
    "# Using averaged word vectors\n",
    "\n",
    "import numpy as np\n",
    "norms = np.sum(weights_0_1 * weights_0_1, axis = 1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "# prenormalized all the word embeddings into a matrix called normed_weights\n",
    "normed_weights = weights_0_1 * norms \n",
    "\n",
    "# Converting each reveiw (list of words) into embeddings using the average approach.\n",
    "def make_sent_vect(words):\n",
    "    indices = list(map(lambda x: word2index[x], filter(lambda x: x in word2index, words)))\n",
    "    return np.mean(normed_weights[indices], axis = 0)\n",
    "\n",
    "reviews2vectors = list()\n",
    "for review in tokens: # Tokenized reviews\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "\n",
    "reviews2vectors = np.array(reviews2vectors)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Counter()\n",
    "    for i, val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    most_similar - list()\n",
    "    \n",
    "    for idx, score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0 : 40])\n",
    "    return most_similar\n",
    "\n",
    "most_similar_reviews(['boring', 'awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[1. 2. 3.]\n",
      "[0.1 0.2 0.3]\n",
      "[-1.  -0.5  0. ]\n",
      "[0. 0. 0.]\n",
      "[13 15 17]\n",
      "[13. 15. 17.]\n"
     ]
    }
   ],
   "source": [
    "# Working with Identity Matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([0.1, 0.2, 0.3])\n",
    "c = np.array([-1, -0.5, 0])\n",
    "d = np.array([0, 0, 0])\n",
    "\n",
    "identity = np.eye(3)\n",
    "print(identity)\n",
    "\n",
    "# this multiplication will do nothing\n",
    "print(a.dot(identity))\n",
    "print(b.dot(identity))\n",
    "print(c.dot(identity))\n",
    "print(d.dot(identity))\n",
    "\n",
    "this = np.array([2, 4, 6])\n",
    "movie = np.array([10, 10, 10])\n",
    "rocks = np.array([1, 1, 1])\n",
    "\n",
    "print(this + movie + rocks)\n",
    "print(this.dot(identity) + movie.dot(identity) + rocks.dot(identity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      "  0.11111111 0.11111111 0.11111111]]\n"
     ]
    }
   ],
   "source": [
    "# toy example with forward propagation\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp / np.sum(temp, axis = 1, keepdims = True)\n",
    "\n",
    "word_vects = {}\n",
    "word_vects['yankees'] = np.array([0., 0., 0.]) # word embedding\n",
    "word_vects['bears'] = np.array([0., 0., 0.]) # word embedding\n",
    "word_vects['braves'] = np.array([0., 0., 0.]) # ...\n",
    "word_vects['red'] = np.array([0., 0., 0.])\n",
    "word_vects['sox'] = np.array([0., 0., 0.])\n",
    "word_vects['lose'] = np.array([0., 0., 0.])\n",
    "word_vects['defeat'] = np.array([0., 0., 0.])\n",
    "word_vects['beat'] = np.array([0., 0., 0.])\n",
    "word_vects['tie'] = np.array([0., 0., 0.]) # word embedding\n",
    "\n",
    "# Sentence embedding to output classification weights\n",
    "sent2output = np.random.rand(3, len(word_vects))\n",
    "\n",
    "# Transition weights\n",
    "identity = np.eye(3)\n",
    "\n",
    "# Creates a sentence embedding\n",
    "layer_0 = word_vects['red']\n",
    "layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "# Predicts over all vocabulary\n",
    "pred = softmax(layer_2.dot(sent2output))\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets the one-hot vector for 'yankees'\n",
    "y = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "pred_delta = pred - y\n",
    "layer_2_delta = pred_delta.dot([sent2output.T])\n",
    "defeat_delta = layer_2_delta * 1\n",
    "layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "sox_delta = layer_1_delta * 1\n",
    "layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "alpha = 0.01\n",
    "word_vects['red'] -= layer_0_delta * alpha\n",
    "word_vects['sox'] -= sox_delta * alpha\n",
    "word_vects['defeat'] -= defeat_delta * alpha\n",
    "identity -= np.outer(layer_0, layer_1_delta) * alpha\n",
    "identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "sent2output -= np.outer(layer_2, pred_delta) * alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data using the terminal\n",
    "# wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-1.tar.gz\n",
    "# tar -xvf tasks_1-20_v1-1.tar.gz\n",
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "f = open('tasksv11/en/qal_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0 : 1000]:\n",
    "    tokens.append(line.lower().replace('\\n', '').split(' ')[1 : ])\n",
    "\n",
    "print(tokens[0 : 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "vocab = set()\n",
    "\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "        \n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) - 0.5) * 0.1 # Word embeddings\n",
    "recurrent = np.eye(embed_size) # Embedding -> embedding (initially the identity matrix)\n",
    "start = np.zeros(embed_size) # Sentence embedding for an empty sentence\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1 # Embedding -> output weights\n",
    "one_hot = np.eye(len(vocab)) # One-hot lookups (for the loss function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forwar propagation with arbitrary length\n",
    "\n",
    "def predict(sent):\n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    preds = list() # Forward propagates\n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder)) # Tries to predict the next term\n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        \n",
    "        # Generates the next hidden state\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
    "        layers.append(layer)\n",
    "        \n",
    "    return layers, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words2indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fb513a2db00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords2indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words2indices' is not defined"
     ]
    }
   ],
   "source": [
    "# Backpropagation with arbitrary length\n",
    "\n",
    "for iter in range(30000): # Forward\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter % len(tokens)][1 : ])\n",
    "    layers, loss = predict(sent)\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))): # Backpropagates\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx - 1]\n",
    "        \n",
    "        if (layer_idx > 0): # If not the first layer\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "            \n",
    "            # If the last layer, don't pull from a later one, because it doesn't exist\n",
    "            if (layer_idx == len(layers) - 1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n",
    "                \n",
    "        else: # if the first layer\n",
    "            layer['hidden_delta'] = layers[layer_idx + 1]['hidden_delta'].dot(recurrent.transpose())\n",
    "            \n",
    "    # Updating weights\n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    for layer_idx, layer in enumerate(layers[1 : ]):\n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "        embed_idx = sent[layer_idx]\n",
    "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))\n",
    "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
    "    \n",
    "    if (iter % 1000 == 0):\n",
    "        print(\"Perplexity: \" + str(np.exp(loss / len(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_index = 4\n",
    "\n",
    "l, _ = predict(words2indices(tokens[sent_index]))\n",
    "\n",
    "print(tokens[sent_index])\n",
    "\n",
    "for i, each_layer in enumerate(l[1 : -1]):\n",
    "    input = tokens[sent_index][i]\n",
    "    true = tokens[sent_index][i + 1]\n",
    "    pred = vocab[each_layer['pred'].argmax()]\n",
    "    print('Prev Input: ' + input + (' ' * (12 - len(input))) +\n",
    "          ' True: ' + true + (' ' * (15 - len(true))) + ' Pred: ' + pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
