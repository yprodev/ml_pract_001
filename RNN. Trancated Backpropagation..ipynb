{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'shakespear.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8365a7b366b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shakespear.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shakespear.txt'"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "np.random.seed()\n",
    "\n",
    "f = open('shakespear.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "indices = np.array(list(map(lambda x: word2index[x], raw)))\n",
    "\n",
    "\n",
    "embed = Embedding(vocab_size = len(vocab), dim = 512)\n",
    "model = RNNCell(n_inputs = 512, n_hidden = 512, n_output = len(vocab))\n",
    "\n",
    "criterion = CrossEntropy()\n",
    "optim = SGD(parameters = model.get_parameters() + embed.get_parameters(), alpha = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "bptt = 16 # Backpropagation truncation length\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[ : n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batches_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0 : -1]\n",
    "target_batched_indices = batched_indices[1 : ]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[ : n_bptt * bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[ : n_bptt * bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
    "\n",
    "print(raw[0 : 5])\n",
    "print(indices[0 : 5])\n",
    "print(batched_indices[0 : 5])\n",
    "print(input_batches[0][0 ; 5])\n",
    "print(target_batches[0][0 ; 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated Backpropagation in practice\n",
    "def train(iterations = 100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size = batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd = True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd = True)\n",
    "                rnn_input = embed.forward(input = input)\n",
    "                output, hidden = model.forward(input = rnn_input, hidden = hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd = True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if (t == 0):\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "                \n",
    "            for loss in losses:\n",
    "                ''\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = '\\r Iter: ' + str(iter)\n",
    "            log += ' - Batch: ' + str(batch_i + 1) + ' / ' + str(len(input_batches))\n",
    "            log += ' - Loss: ' + str(np.exp(total_loss / (batch_i + 1)))\n",
    "            if (batch_i == 0):\n",
    "                log += ' - ' + generate_sample(70, '\\n').replace('\\n', ' ')\n",
    "            if (batch_i % 10 == 0 or batch_i - 1 == len(input_batches)):\n",
    "                sys.stdout.write(log)\n",
    "\n",
    "        optim.alpha *= 0.99\n",
    "        print()\n",
    "\n",
    "        \n",
    "def generate_sample(n = 30, init_char = ' '):\n",
    "    s = ''\n",
    "    hidden = model.init_hidden(batch_size = 1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input_rnn_input, hidden = hidden)\n",
    "        output.data *= 10 # Temperature for sampling; higher = greedier\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = (temp_dist > np.random.rand()).argmax() # Samples from pred\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "print(generate_sample(n = 2000, init_char = '\\n'))\n",
    "        \n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code shows a recurrent backpropagation loop for sigmoid and relu\n",
    "# activations. Notice how the gradients become very small / large for\n",
    "# sigmoid / relu, respectively. During backprop, they become large as the result of the matrix\n",
    "# multiplication, and small as a result of the sigmoid activation having a very flat derivative\n",
    "# at its tails (common for many nonlinearities).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "(sigmoid, relu) = (lambda x: 1 / (1 + np.exp(-x)), lambda x: (x > 0).astype(float) * x)\n",
    "weights = np.array([[1, 4], [4, 1]])\n",
    "activation = sigmoid(np.array([1, 0.01]))\n",
    "\n",
    "print('Sigmoid Activations')\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "print('\\nSigmoid Gradients')\n",
    "gradient = np.ones_like(activation)\n",
    "\n",
    "# The derivative of sigmoid causes very small\n",
    "# gradients when activation is very near\n",
    "# 0 or 1 (the tails)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient)\n",
    "    gradient = gradient.dot(weights.transpose())\n",
    "    print(gradient)\n",
    "    \n",
    "print('Activations')\n",
    "activations = list()\n",
    "\n",
    "for iter in range(10):\n",
    "    # The matrix multiplication causes exploding gradients\n",
    "    # that don't get squished by a nonlinearity\n",
    "    # (as in sigmoid)\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "    \n",
    "print('\\nGradients')\n",
    "\n",
    "gradient = np.ones_like(activation)\n",
    "\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long short-term memory (LSTM) cells\n",
    "\n",
    "def forward(self, input, hidden):\n",
    "    from_prev_hidden = self.w_hh.forward(hidden)\n",
    "    combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "    new_hidden = self.activation.forward(combined)\n",
    "    output = self.w_ho.forward(new_hidden)\n",
    "    return output, new_hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, input, hidden):\n",
    "    prev_hidden, prev_cell = (hidden[0], hidden[1])\n",
    "    \n",
    "    f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid() # forget\n",
    "    i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid() # input\n",
    "    o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid() # output\n",
    "    u = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh() # update\n",
    "    \n",
    "    cell = (f * prev_cell) + (i * u)\n",
    "    h = o * cell.tanh()\n",
    "    output = self.w_ho.forward(h)\n",
    "    return output, (h, cell)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_hidden, n_hidden, bias = False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias = False)        \n",
    "        self.ho = Linear(n_hidden, n_hidden, bias = False)        \n",
    "        self.hc = Linear(n_hidden, n_hidden, bias = False)     \n",
    "        \n",
    "        self.w_ho = Linear(n_hidden, n_output, bias = False)\n",
    "        \n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()        \n",
    "        self.parameters += self.xo.get_parameters()        \n",
    "        self.parameters += self.xc.get_parameters()        \n",
    "        self.parameters += self.hf.get_parameters()        \n",
    "        self.parameters += self.hi.get_parameters()        \n",
    "        self.parameters += self.ho.get_parameters()        \n",
    "        self.parameters += self.hc.get_parameters()        \n",
    "        \n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "    \n",
    "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid() # forget\n",
    "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid() # input\n",
    "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid() # output\n",
    "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh() # update\n",
    "        c = (f * prev_cell) + (i * g) # Cell\n",
    "        h = o * c.tanh()\n",
    "        \n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd = True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd = True)\n",
    "        h.data[ : , 0] += 1\n",
    "        c.data[ : , 0] += 1\n",
    "        \n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swapping out the vanilla RNN with the new LSTM cell\n",
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "f = open('shakespear.txt', 'r')\n",
    "raw = f.read()\n",
    "f.close()\n",
    "\n",
    "vocab = list(set(raw))\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "indices = np.array(list(map(lambda x: word2index[x], raw)))\n",
    "\n",
    "embed = Embedding(vocab_size = len(vocab), dim = 512)\n",
    "model = LSTMCell(n_inputs = 512, n_hidden = 512, n_output = len(vocab))\n",
    "model.w_ho.weight.data *= 0 # This seemed to help training\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters = model.get_parameters() + embed.get_parameters(), alpha = 0.05)\n",
    "\n",
    "batch_size = 16\n",
    "bptt = 25\n",
    "n_batches = int((indices.shape[0] / (batch_size)))\n",
    "\n",
    "trimmed_indices = indices[ : n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches)\n",
    "batch_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0 : -1]\n",
    "target_batched_indices = batched_indices[1 : ]\n",
    "\n",
    "n_btpp = int(((n_batches - 1) / bptt))\n",
    "input_batches = input_batched_indices[ : n_bptt * bptt]\n",
    "input_batches = input_batches.reshape(n_bptt, bptt, batch_size)\n",
    "target_batches = target_batched_indices[ : n_bptt * bptt]\n",
    "target_batches = target_batches.reshape(n_bptt, bptt, batch_size)\n",
    "\n",
    "min_loss = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the LSTM character language model\n",
    "\n",
    "for iter in range(iterations):\n",
    "    total_loss, n_loss = (0, 0)\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size = batch_size)\n",
    "    batches_to_train = len(input_batches)\n",
    "    \n",
    "    for batch_i in range(batches_to_train):\n",
    "        hidden = (Tensor(hidden[0].data, autograd = True), Tensor(hidden[1].data, autograd = True))\n",
    "        \n",
    "        losses = list()\n",
    "        \n",
    "        for t in range(bptt):\n",
    "            input = Tensor(input_batches[batch_i][t], autograd = True)\n",
    "            rnn_input = embed.forward(input = input)\n",
    "            output, hidden = model.forward(input = rnn_input, hidden = hidden)\n",
    "            \n",
    "            target = Tensor(target_batches[batch_i][t], autograd = True)\n",
    "            batch_loss = criterion.forward(output, target)\n",
    "            \n",
    "            if (t == 0):\n",
    "                losses.append(batch_loss)\n",
    "            else:\n",
    "                losses.append(batch_loss + losses[-1])\n",
    "        \n",
    "        loss = losses[-1]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.data / bptt\n",
    "        epoch_loss = np.exp(total_loss / (batch_i + 1))\n",
    "        if (epoch_loss < min_loss):\n",
    "            min_loss = epoch_loss\n",
    "            print()\n",
    "            \n",
    "        \n",
    "        log = \"\\r Iter: \" + str(iter)\n",
    "        log += \" - Alpha: \" + str(optim.alpha)[0 : 5]\n",
    "        log += \" - Batch: \" + str(batch_i + 1) + \" / \" + str(len(input_batches))\n",
    "        log += \" - Min Loss: \" + str(min_loss)[0 : 5]\n",
    "        log += \" - Loss: \" + str(epoch_loss)\n",
    "        if (batch_i == 0):\n",
    "            s = generate_sample(n = 70, init_char = 'T').replace('\\n', ' ')\n",
    "            log += ' - ' + s\n",
    "        sys.stdout.write(log)\n",
    "        \n",
    "    optim.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n = 30, init_char = ' '):\n",
    "    s = ''\n",
    "    hidden = model.init_hidden(batch_size = 1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input = rnn_input, hidden = hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = output.data.argmax() # Takes the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "print(generate_sample(n = 500, init_char = '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
